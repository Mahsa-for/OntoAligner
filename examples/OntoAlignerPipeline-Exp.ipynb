{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058bdc04-c308-4f05-a6f1-99d87173c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ontoaligner\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0b239-c1e1-4bb4-bb14-9b318e375633",
   "metadata": {},
   "source": [
    "## MaterialInformation-MatOnto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94b0f6-a644-4cab-90c8-b4c825aa9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ontoaligner.OntoAlignerPipeline(\n",
    "    task_class=ontoaligner.ontology.MaterialInformationMatOntoOMDataset,\n",
    "    source_ontology_path=\"assets/MI-MatOnto/mi_ontology.xml\",\n",
    "    target_ontology_path=\"assets/MI-MatOnto/matonto_ontology.xml\",\n",
    "    reference_matching_path=\"assets/MI-MatOnto/matchings.xml\",\n",
    "    output_dir=\"results\",\n",
    "    output_format=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65522cf-c8f2-4927-afee-b336a7b46352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bde1604a280499283849d3a10e172a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbeca0603360424e8b7d12f24559c573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1bc8f1c3644c3497f7fd00c2803bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "545it [00:00, 9972.54it/s]\n",
      "100%|██████████| 545/545 [00:00<00:00, 425996.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of random_positive_examples examples: 1\n",
      "No of random_negative_examples examples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/86 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "100%|██████████| 86/86 [05:30<00:00,  3.84s/it]\n",
      "100%|██████████| 444/444 [00:00<00:00, 176837.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_threshold: 0.4\n",
      "llm_threshold: 0.4\n",
      "EVAL: {'intersection': 102, 'precision': 65.38461538461539, 'recall': 33.77483443708609, 'f-score': 44.54148471615721, 'predictions-len': 156, 'reference-len': 302}\n",
      "----------------------------------------\n",
      "RS: 342.7561044692993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intersection': 102,\n",
       " 'precision': 65.38461538461539,\n",
       " 'recall': 33.77483443708609,\n",
       " 'f-score': 44.54148471615721,\n",
       " 'predictions-len': 156,\n",
       " 'reference-len': 302}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_path='mistralai/Mistral-7B-v0.3'\n",
    "retriever_path='all-MiniLM-L6-v2'\n",
    "method = \"fewshot-rag\"\n",
    "ir_threshold = 0.4\n",
    "llm_threshold = 0.4\n",
    "\n",
    "start = time.time()\n",
    "matchings, evaluation = pipeline(\n",
    "    method=method, \n",
    "    encoder_model=ontoaligner.encoder.ConceptChildrenFewShotEncoder(),\n",
    "    model_class=ontoaligner.ontology_matchers.MistralLLMBERTRetrieverFSRAG, \n",
    "    postprocessor=ontoaligner.postprocess.rag_hybrid_postprocessor,\n",
    "    llm_path=llm_path, \n",
    "    retriever_path=retriever_path, \n",
    "    llm_threshold=llm_threshold,\n",
    "    ir_rag_threshold=ir_threshold,\n",
    "    n_shots=2,\n",
    "    top_k=5,\n",
    "    max_length=512,\n",
    "    max_new_tokens=10,\n",
    "    device='cuda', \n",
    "    batch_size=32, \n",
    "    return_matching=True, \n",
    "    evaluate=True,\n",
    ")\n",
    "print(\"ir_threshold:\", ir_threshold)\n",
    "print(\"llm_threshold:\", llm_threshold)\n",
    "print(\"EVAL:\", evaluation)\n",
    "print(\"----\"*10)\n",
    "print(\"RS:\", time.time() - start)\n",
    "\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93c601-96e5-4f40-81b1-178d7980857f",
   "metadata": {},
   "source": [
    "# Fish-Zooplankton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4129a0-f337-4691-a0b2-0d4429d22409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ontoaligner.OntoAlignerPipeline(\n",
    "    task_class=ontoaligner.ontology.FishZooplanktonOMDataset,\n",
    "    source_ontology_path=\"assets/fish-zooplankton/source.xml\",\n",
    "    target_ontology_path=\"assets/fish-zooplankton/target.xml\",\n",
    "    reference_matching_path=\"assets/fish-zooplankton/reference.xml\",\n",
    "    output_dir=\"results\",\n",
    "    output_format=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "421039f0-b59f-48ae-a4cd-82501df79965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6b897df396488c9546134e539542f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e810d37c4d64ca5a343b9854231413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b672052f6d4ea2a53d2f39fa37047b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2737it [00:00, 16323.80it/s]\n",
      "100%|██████████| 2737/2737 [00:00<00:00, 727076.45it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]/nfs/home/babaeih/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [01:47<00:00,  1.79s/it]\n",
      "100%|██████████| 7592/7592 [00:00<00:00, 39242.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_threshold: 0.7\n",
      "llm_threshold: 0.95\n",
      "EVAL: {'intersection': 1291, 'precision': 87.7038043478261, 'recall': 85.15831134564644, 'f-score': 86.41231593038823, 'predictions-len': 1472, 'reference-len': 1516}\n",
      "----------------------------------------\n",
      "RS: 116.54034519195557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intersection': 1291,\n",
       " 'precision': 87.7038043478261,\n",
       " 'recall': 85.15831134564644,\n",
       " 'f-score': 86.41231593038823,\n",
       " 'predictions-len': 1472,\n",
       " 'reference-len': 1516}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "method = \"rag\"\n",
    "llm_path='meta-llama/Llama-3.2-3B'\n",
    "retriever_path='all-MiniLM-L6-v2'\n",
    "ir_threshold = 0.7\n",
    "llm_threshold = 0.95\n",
    "huggingface_access_token= \"\"\n",
    "\n",
    "matchings, evaluation = pipeline(\n",
    "    method=method,\n",
    "    encoder_model=ontoaligner.encoder.ConceptRAGEncoder(),\n",
    "    model_class=ontoaligner.ontology_matchers.LLaMALLMBERTRetrieverRAG,\n",
    "    postprocessor=ontoaligner.postprocess.rag_hybrid_postprocessor,\n",
    "    llm_path=llm_path,\n",
    "    retriever_path=retriever_path,\n",
    "    llm_threshold=llm_threshold,\n",
    "    ir_rag_threshold=ir_threshold,\n",
    "    top_k=5,\n",
    "    max_length=512,\n",
    "    max_new_tokens=10,\n",
    "    device='cuda',\n",
    "    batch_size=128,\n",
    "    return_matching=True,\n",
    "    evaluate=True,\n",
    "    huggingface_access_token=huggingface_access_token,\n",
    ")\n",
    "print(\"ir_threshold:\", ir_threshold)\n",
    "print(\"llm_threshold:\", llm_threshold)\n",
    "print(\"EVAL:\", evaluation)\n",
    "print(\"----\"*10)\n",
    "print(\"RS:\", time.time() - start)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec02c37-ede5-4fe2-8379-3d69e627a5e8",
   "metadata": {},
   "source": [
    "# Mouse-Human "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d29bd-2d6d-49a8-93dc-3ec6bd1b12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ontoaligner.OntoAlignerPipeline(\n",
    "    task_class=ontoaligner.ontology.MouseHumanOMDataset,\n",
    "    source_ontology_path=\"assets/mouse-human/source.xml\",\n",
    "    target_ontology_path=\"assets/mouse-human/target.xml\",\n",
    "    reference_matching_path=\"assets/mouse-human/reference.xml\",\n",
    "    output_dir=\"results\",\n",
    "    output_format=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367a78d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6b897df396488c9546134e539542f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e810d37c4d64ca5a343b9854231413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b672052f6d4ea2a53d2f39fa37047b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2737it [00:00, 16323.80it/s]\n",
      "100%|██████████| 2737/2737 [00:00<00:00, 727076.45it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]/nfs/home/babaeih/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nfs/home/babaeih/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [01:47<00:00,  1.79s/it]\n",
      "100%|██████████| 7592/7592 [00:00<00:00, 39242.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_threshold: 0.7\n",
      "llm_threshold: 0.95\n",
      "EVAL: {'intersection': 1291, 'precision': 87.7038043478261, 'recall': 85.15831134564644, 'f-score': 86.41231593038823, 'predictions-len': 1472, 'reference-len': 1516}\n",
      "----------------------------------------\n",
      "RS: 116.54034519195557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intersection': 1291,\n",
       " 'precision': 87.7038043478261,\n",
       " 'recall': 85.15831134564644,\n",
       " 'f-score': 86.41231593038823,\n",
       " 'predictions-len': 1472,\n",
       " 'reference-len': 1516}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "method = \"rag\"\n",
    "llm_path='meta-llama/Llama-3.2-3B'\n",
    "retriever_path='all-MiniLM-L6-v2'\n",
    "ir_threshold = 0.7\n",
    "llm_threshold = 0.95\n",
    "huggingface_access_token= \"\"\n",
    "\n",
    "matchings, evaluation = pipeline(\n",
    "    method=method,\n",
    "    encoder_model=ontoaligner.encoder.ConceptRAGEncoder(),\n",
    "    model_class=ontoaligner.ontology_matchers.LLaMALLMBERTRetrieverRAG,\n",
    "    postprocessor=ontoaligner.postprocess.rag_hybrid_postprocessor,\n",
    "    llm_path=llm_path,\n",
    "    retriever_path=retriever_path,\n",
    "    llm_threshold=llm_threshold,\n",
    "    ir_rag_threshold=ir_threshold,\n",
    "    top_k=5,\n",
    "    max_length=512,\n",
    "    max_new_tokens=10,\n",
    "    device='cuda',\n",
    "    batch_size=128,\n",
    "    return_matching=True,\n",
    "    evaluate=True,\n",
    "    huggingface_access_token=huggingface_access_token,\n",
    ")\n",
    "print(\"ir_threshold:\", ir_threshold)\n",
    "print(\"llm_threshold:\", llm_threshold)\n",
    "print(\"EVAL:\", evaluation)\n",
    "print(\"----\"*10)\n",
    "print(\"RS:\", time.time() - start)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94dcdf3-887c-46be-9444-d9de66eea974",
   "metadata": {},
   "source": [
    "# Macroalgae-Macrozoobenthos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e84bc-98a9-4849-ad14-17fd3fbf2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ontoaligner.OntoAlignerPipeline(\n",
    "    task_class=ontoaligner.ontology.MacroalgaeMacrozoobenthosOMDataset,\n",
    "    source_ontology_path=\"assets/macroalgae-macrozoobenthos/source.xml\",\n",
    "    target_ontology_path=\"assets/macroalgae-macrozoobenthos/target.xml\",\n",
    "    reference_matching_path=\"assets/macroalgae-macrozoobenthos/reference.xml\",\n",
    "    output_dir=\"results\",\n",
    "    output_format=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d28aff-a5bc-4a49-8265-26bd8d276e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "108it [00:00, 1506.91it/s]\n",
      "100%|██████████| 108/108 [00:00<00:00, 351151.03it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.71it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 153644.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_threshold: 0.0\n",
      "llm_threshold: 0.8\n",
      "EVAL: {'intersection': 12, 'precision': 75.0, 'recall': 66.66666666666666, 'f-score': 70.58823529411765, 'predictions-len': 16, 'reference-len': 18}\n",
      "----------------------------------------\n",
      "RS: 2.804941177368164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intersection': 12,\n",
       " 'precision': 75.0,\n",
       " 'recall': 66.66666666666666,\n",
       " 'f-score': 70.58823529411765,\n",
       " 'predictions-len': 16,\n",
       " 'reference-len': 18}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QwenLLMTFIDFRetrieverRAG(ontoaligner.ontology_matchers.RAG):\n",
    "    Retrieval = ontoaligner.ontology_matchers.TFIDFRetrieval\n",
    "    LLM = ontoaligner.ontology_matchers.AutoModelDecoderRAGLLMV2\n",
    "\n",
    "    def __str__(self):\n",
    "        return super().__str__() + \"-QwenLLMTFIDFRetrieverRAG\"\n",
    "    \n",
    "    \n",
    "start = time.time()\n",
    "method = \"rag\"\n",
    "llm_threshold = 0.8\n",
    "ir_threshold = 0.2\n",
    "llm_path='Qwen/Qwen2-0.5B'\n",
    "\n",
    "matchings, evaluation = pipeline(\n",
    "    method=method,\n",
    "    encoder_model=ontoaligner.encoder.ConceptRAGEncoder(),\n",
    "    model_class=QwenLLMTFIDFRetrieverRAG,\n",
    "    postprocessor=ontoaligner.postprocess.rag_hybrid_postprocessor,\n",
    "    llm_path=llm_path,\n",
    "    llm_threshold=llm_threshold,\n",
    "    ir_rag_threshold=ir_threshold,\n",
    "    top_k=5,\n",
    "    max_length=512,\n",
    "    max_new_tokens=10,\n",
    "    device='cuda',\n",
    "    batch_size=64,\n",
    "    return_matching=True,\n",
    "    evaluate=True\n",
    ")\n",
    "print(\"ir_threshold:\", ir_threshold)\n",
    "print(\"llm_threshold:\", llm_threshold)\n",
    "print(\"EVAL:\", evaluation)\n",
    "print(\"----\"*10)\n",
    "t = time.time() - start\n",
    "print(\"RS:\", t)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32aee6b-d310-4076-bfff-c9eda1ca2779",
   "metadata": {},
   "source": [
    "# Nell-Dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058d334-43dc-4eb0-85c0-ba4d6e634d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ontoaligner.OntoAlignerPipeline(\n",
    "    task_class=ontoaligner.ontology.NellDbpediaOMDataset,\n",
    "    source_ontology_path=\"assets/nell-dbpedia/source.xml\",\n",
    "    target_ontology_path=\"assets/nell-dbpedia/target.xml\",\n",
    "    reference_matching_path=\"assets/nell-dbpedia/reference.xml\",\n",
    "    output_dir=\"results\",\n",
    "    output_format=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "254f197d-82b2-48b3-b7f0-f17f581d882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f10f177039043e894a4f5e597dc0f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34dbf8ea514132a9cfefe13b5e565d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [00:00, 23359.80it/s]\n",
      "100%|██████████| 134/134 [00:00<00:00, 264986.67it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|██████████| 670/670 [00:00<00:00, 314408.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_threshold: 0.0\n",
      "llm_threshold: 0.7\n",
      "EVAL: {'intersection': 126, 'precision': 97.67441860465115, 'recall': 97.67441860465115, 'f-score': 97.67441860465115, 'predictions-len': 129, 'reference-len': 129}\n",
      "----------------------------------------\n",
      "RS: 5.911125183105469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intersection': 126,\n",
       " 'precision': 97.67441860465115,\n",
       " 'recall': 97.67441860465115,\n",
       " 'f-score': 97.67441860465115,\n",
       " 'predictions-len': 129,\n",
       " 'reference-len': 129}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QwenLLMBERTRetrieverRAG(ontoaligner.ontology_matchers.RAG):\n",
    "    Retrieval = ontoaligner.ontology_matchers.SBERTRetrieval\n",
    "    LLM = ontoaligner.ontology_matchers.AutoModelDecoderRAGLLMV2\n",
    "\n",
    "    def __str__(self):\n",
    "        return super().__str__() + \"-QwenLLMBERTRetrieverRAG\"\n",
    "    \n",
    "    \n",
    "start = time.time()\n",
    "method = \"rag\"\n",
    "llm_threshold = 0.7\n",
    "ir_threshold = 0.2\n",
    "llm_path='Qwen/Qwen2-0.5B'\n",
    "ir_path='sentence-transformers/sentence-t5-base'\n",
    "\n",
    "matchings, evaluation = pipeline(\n",
    "    method=method,\n",
    "    encoder_model=ontoaligner.encoder.ConceptRAGEncoder(),\n",
    "    model_class=QwenLLMBERTRetrieverRAG,\n",
    "    postprocessor=ontoaligner.postprocess.rag_hybrid_postprocessor,\n",
    "    llm_path=llm_path,\n",
    "    retriever_path=ir_path,\n",
    "    llm_threshold=llm_threshold,\n",
    "    ir_rag_threshold=ir_threshold,\n",
    "    top_k=5,\n",
    "    max_length=512,\n",
    "    max_new_tokens=10,\n",
    "    device='cuda',\n",
    "    batch_size=2048,\n",
    "    return_matching=True,\n",
    "    evaluate=True\n",
    ")\n",
    "print(\"ir_threshold:\", ir_threshold)\n",
    "print(\"llm_threshold:\", llm_threshold)\n",
    "print(\"EVAL:\", evaluation)\n",
    "print(\"----\"*10)\n",
    "t = time.time() - start\n",
    "print(\"RS:\", t)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501b2e8-335e-4ee5-9903-a897b3a2815b",
   "metadata": {},
   "source": [
    "# Yago-Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424549b-1b5d-47f3-8584-1ac874e24cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ontoaligner.OntoAlignerPipeline(\n",
    "    task_class=ontoaligner.ontology.YagoWikidataOMDataset,\n",
    "    source_ontology_path=\"assets/yago-wikidata/source.xml\",\n",
    "    target_ontology_path=\"assets/yago-wikidata/target.xml\",\n",
    "    reference_matching_path=\"assets/yago-wikidata/reference.xml\",\n",
    "    output_dir=\"results\",\n",
    "    output_format=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ac6182-107a-4716-af71-336593f15566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00292f96be149289321553011038463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4259b6e53594d8da2390b662ccc6473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f8b84a3bb246d5b13efa95cc165cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [00:00, 8668.63it/s]\n",
      "100%|██████████| 304/304 [00:00<00:00, 414386.88it/s]\n",
      "100%|██████████| 24/24 [00:26<00:00,  1.10s/it]\n",
      "100%|██████████| 1519/1519 [00:00<00:00, 214791.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_threshold: 0.0\n",
      "llm_threshold: 0.5\n",
      "EVAL: {'intersection': 283, 'precision': 99.29824561403508, 'recall': 93.0921052631579, 'f-score': 96.09507640067912, 'predictions-len': 285, 'reference-len': 304}\n",
      "----------------------------------------\n",
      "RS: 33.73451352119446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intersection': 283,\n",
       " 'precision': 99.29824561403508,\n",
       " 'recall': 93.0921052631579,\n",
       " 'f-score': 96.09507640067912,\n",
       " 'predictions-len': 285,\n",
       " 'reference-len': 304}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MinistralLLMBERTRetrieverRAG(ontoaligner.ontology_matchers.RAG):\n",
    "    Retrieval = ontoaligner.ontology_matchers.SBERTRetrieval\n",
    "    LLM = ontoaligner.ontology_matchers.AutoModelDecoderRAGLLM\n",
    "\n",
    "    def __str__(self):\n",
    "        return super().__str__() + \"-MinistralLLMBERTRetrieverRAG\"\n",
    "    \n",
    "\n",
    "start = time.time()\n",
    "method = \"rag\"\n",
    "llm_threshold = 0.5\n",
    "ir_threshold = 0.2\n",
    "llm_path='ministral/Ministral-3b-instruct'\n",
    "ir_path='sentence-transformers/sentence-t5-base'\n",
    "\n",
    "matchings, evaluation = pipeline(\n",
    "    method=method,\n",
    "    encoder_model=ontoaligner.encoder.ConceptRAGEncoder(),\n",
    "    model_class=MinistralLLMBERTRetrieverRAG,\n",
    "    postprocessor=ontoaligner.postprocess.rag_hybrid_postprocessor,\n",
    "    llm_path=llm_path,\n",
    "    retriever_path=ir_path,\n",
    "    llm_threshold=llm_threshold,\n",
    "    ir_rag_threshold=ir_threshold,\n",
    "    top_k=5,\n",
    "    max_length=512,\n",
    "    max_new_tokens=10,\n",
    "    device='cuda',\n",
    "    batch_size=64,\n",
    "    return_matching=True,\n",
    "    evaluate=True\n",
    ")\n",
    "print(\"ir_threshold:\", ir_threshold)\n",
    "print(\"llm_threshold:\", llm_threshold)\n",
    "print(\"EVAL:\", evaluation)\n",
    "print(\"----\"*10)\n",
    "t = time.time() - start\n",
    "print(\"RS:\", t)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d240b-8f1f-40b8-8a25-7d04e53702e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
